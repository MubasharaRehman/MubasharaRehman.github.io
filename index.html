

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Mubashara Rehman</title>
  <link rel="stylesheet" href="stylesheet.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
   <style>
  .theme-icon {
    font-size: 20px;
    cursor: pointer;
    position: absolute;
    right: 20px;
    top: 16px;
    color: #333;
    transition: color 0.3s ease;
  }

  body.dark-mode .theme-icon {
    color: #f0f0f0;
  }

  body.dark-mode {
    background-color: #121212;
    color: #eee;
  }

  .dark-mode .navbar {
    background-color: #1e1e1e;
  }

  .dark-mode .navbar a {
    color: #eee;
  }

  .dark-mode .btn {
    background-color: #1e1e1e !important;
    color: #ffdddd !important;
    border-color: #eee !important;
  }

  .dark-mode .publication-item {
    background-color: #1e1e1e;
    border-color: #333;
  }

  .dark-mode .publication-text p a {
    color: #ffa500;
  }

  .dark-mode .btn:hover {
    background-color: #eee !important;
    color: #550000 !important;
  }

  .dark-mode .contact-icons a {
    background-color: transparent;
    border-color: #eee;
    color: #f09228;
  }
     .theme-icon {
  font-size: 20px;
  margin-right: 10px;
  cursor: pointer;
  color: #444;
  transition: color 0.3s;
}

body.dark-mode .theme-icon {
  color: #f1c40f; /* Yellowish in dark mode */
}
</style>

 

</head>
<body>

  <!-- Navbar -->
<nav class="navbar">
  <a href="#home" class="active">Home</a>
  <a href="#Publications">Publications</a>
  <a href="#Achievements">Achievements</a>
     <!-- Theme Toggle Switch -->
    <!-- Theme Toggle with Icon -->
 <i id="theme-icon" class="fas fa-moon theme-icon" title="Toggle dark mode"></i>
 
</nav>

  

  <!-- Home Section -->
  <section id="home" class="section-container home-grid">
  <div class="home-text">
<h1><strong>Mubashara</strong> Rehman</h1>
    <!-- <p>I’m currently pursuing my PhD in Computer Science and Artificial Intelligence at the  <a href="https://machinelearning.uniud.it/">Machine Learning and Perceptron Lab</a>, <a href="https://www.uniud.it/en/uniud-international?set_language=en">University of Udine</a>, Italy under the supervision of <a href="https://users.dimi.uniud.it/~niki.martinel/">Dr Niki Martinel</a>.My research focused on low-level vision tasks, particularly image and video super-resolution. During my PhD, I developed an efficient implicit deep learning-based image super-resolution technique aimed at significantly reducing model parameters, FLOPs, and inference time while maintaining  model performance.</p> -->
    <p>
I am currently pursuing my Ph.D. in Computer Science and Artificial Intelligence at the 
<a href="https://www.uniud.it/en/uniud-international?set_language=en">University of Udine</a>, Italy, 
in collaboration with the 
<a href="https://www.cro.it/">National Cancer Research Institute, CRO IRCCS</a>, supervised by <a href="https://users.dimi.uniud.it/~christian.micheloni/">Prof. Christian Micheloni</a>, and co-supervised by <a href="https://users.dimi.uniud.it/~niki.martinel/">Dr. Niki Martinel</a>.
My research focuses on deep learning-based metal artifact reduction and domain transformation in CT imaging, 
supported by the Italian Ministry of Health. 
I hold a Master’s degree from <a href="https://en.tju.edu.cn/">Tianjin University</a>, China, and a Bachelor’s degree from Pakistan, 
which laid the foundation for my expertise in computer vision and artificial intelligence. 
My work aims to advance AI-driven CT reconstruction and enhancement, 
bridging technical innovations with real-world healthcare impact.
</p>

    
    
  <p class="contact-icons">
    <a href="mailto:rehman.mubashara@gmail.com" title="Email"><i class="fas fa-envelope"></i></a>
    <a href="data/CV_Mubashara_.pdf">CV</a>
    <a href="https://scholar.google.com/citations?user=WYLi7mgAAAAJ&hl=en" title="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
    <a href="https://github.com/MubasharaRehman" title="Github"><i class="fab fa-github"></i></a>
    <a href="https://orcid.org/my-orcid?orcid=0009-0007-2935-0409" title="ORCID"><i class="fa-brands fa-orcid"></i></a>
<a href="https://www.linkedin.com/in/mubashara-rehman/" title="LinkedIn"><i class="fab fa-linkedin"></i></a>

  </p>
  </div>
  <div class="home-image">
    <img src="images/Mub_DP_github1.jpeg" alt="Profile Photo">
  </div>
</section>

  <!-- Publications Section -->
 <section id="Publications" class="section-container">
 <h2>
  <i class="fas fa-book"></i> Selected Publications
</h2>



  <!-- Publication 1 -->
  <div class="publication-item hover-video">
    <div class="video-container">
      <video muted autoplay loop>
        <source src="images/tip.mp4" type="video/mp4">
      </video>
      <img src="images/ReMAR_DS_ICIAP2025_.jpg" alt="TIP Paper Thumbnail">
    </div>
    <div class="publication-text">
      <a href="https://arxiv.org/pdf/2506.19531">
      <strong>ReMAR-DS: Metal Artifact Reduction and Domain Transformation from kVCT to MVCT</strong>
      </a>
      <p>
  <strong><u>Mubashara Rehman</u></strong>, Niki Martinel, Michele Avanzo, Riccardo Spizzo, Christian Micheloni        
</p>
          <p><em>23rd International Conference on Image Analysis and Processing (ICIAP), 2025</em></p>
      <p>
        <!-- Blind image super-resolution (SR) aims to recover high-resolution images without knowing the degradation. Existing methods often require ground-truth kernels and heavy networks. This work proposes a lightweight model (PL-IDENet) that implicitly learns degradations using a novel loss and a learnable Wiener filter. The method achieves better accuracy with significantly fewer parameters and computations.</p> -->
       Metal artifacts in kVCT affect clinical accuracy. We propose <b>ReMAR-DS</b>, an encoder–decoder framework with enhanced feature recalibration for metal artifact reduction and kVCT→MVCT domain transformation. By emphasizing artifact-prone regions and preserving anatomical structures, our method generates high-quality MVCT-like reconstructions, improving radiotherapy planning while reducing repeated high-dose scans.</p>
      <p>

  <a href="#" class="btn" title="Will be made available soon">Code</a>
  <a href="#" class="btn" title="Will be made available soon">Dataset</a>
  <a href="#" class="btn" title="Will be made available soon">Bibtex</a>

      </p>
    </div>
  </div>

  <!-- Publication 2 -->
  <div class="publication-item hover-video">
    <div class="video-container">
      <video muted autoplay loop>
        <source src="images/idenet.mp4" type="video/mp4">
      </video>
      <img src="images/idenet.png" alt="IDENet Paper Thumbnail">
    </div>
    <div class="publication-text">
      <a href="https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/papers/Khan_IDENet_Implicit_Degradation_Estimation_Network_for_Efficient_Blind_Super_Resolution_CVPRW_2024_paper.pdf">
        <strong>IDENet: Implicit Degradation Estimation Network for Efficient Blind Super Resolution</strong>
      </a>
      <p> <strong><u>Asif Hussain Khan</u></strong>, Christian Micheloni, Niki Martinel</p>
      <p><em>CVPR Workshop, 2024</em></p>
      <p>Blind image super-resolution (SR) restores high-resolution images from low-resolution inputs with unknown degradations. Existing methods need ground-truth degradation or are computationally heavy. The proposed model uses a novel loss and a learnable Wiener filter to implicitly estimate degradation and efficiently solve deconvolution. It outperforms implicit SR methods and matches explicit ones with much fewer parameters.</p>
      <p>
           <a href="https://github.com/asifhkhan" class="btn">Code</a>
      <a href="https://github.com/asifhkhan/PL_IDENET" class="btn">Dataset</a>
      <a href="data/default.bib" class="btn">Bibtex</a>
      </p>
    </div>
  </div>

  <!-- Publication 3 -->
  <div class="publication-item hover-video">
    <div class="video-container">
      <video muted autoplay loop>
        <source src="images/ntire.mp4" type="video/mp4">
      </video>
      <img src="images/ntire.png" alt="NTIRE Paper Thumbnail">
    </div>
    <div class="publication-text">
      <a href="https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/papers/Chen_NTIRE_2024_Challenge_on_Image_Super-Resolution_x4_Methods_and_Results_CVPRW_2024_paper.pdf">
        <strong>NTIRE 2024 Challenge on Image Super-Resolution (×4): Methods and Results</strong>
      </a>
      <p>Chen, Zheng, et al.</p>
      <p><em>NTIRE Challenge, 2024</em></p>
      <p>The NTIRE 2024 Image Super-Resolution (×4) Challenge focused on enhancing low-resolution images using bicubic downsampling inputs. With no limits on model size or training data, the competition aimed for top PSNR performance on the DIV2K test set. Attracting 199 registrants and 20 final submissions, the challenge advanced state-of-the-art SR techniques and showcased emerging trends.</p>
      <p>
          <a href="https://github.com/zhengchen1999/NTIRE2024_ImageSR_x4" class="btn">Code</a>
      <a href="https://github.com/zhengchen1999/NTIRE2024_ImageSR_x4" class="btn">Dataset</a>
      <a href="data/default.bib" class="btn">Bibtex</a>
      </p>
    </div>
  </div>

  <!-- Publication 4 -->
  <div class="publication-item hover-video">
    <div class="video-container">
      <video muted autoplay loop>
        <source src="images/lbkenet.mp4" type="video/mp4">
      </video>
      <img src="images/lbkenet.png" alt="LBKENet Paper Thumbnail">
    </div>
    <div class="publication-text">
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-43153-1_18">
        <strong>LBKENet: Lightweight Blur Kernel Estimation Network for Blind Image Super-Resolution</strong>
      </a>
      <p> <strong><u>Asif Hussain Khan</u></strong>, Rao Muhammad Umer, Matteo Dunnhofer, Christian Micheloni, Niki Martinel</p>
      <p><em>ICIAP, 2023</em></p>
      <p>Blind image super-resolution (Blind-SR) restores high-resolution images from low-resolution inputs with unknown degradations. Existing methods rely on ground-truth blur kernels, but this work proposes a lightweight, implicit kernel estimation network (LBKENet) that learns without ground-truth supervision. It combines a super-resolver and a blur kernel estimator in an end-to-end framework with a novel loss design. The approach achieves competitive performance with 12× fewer parameters, making it suitable for low-resource device.</p>
      <p>
         <a href="https://github.com/asifhkhan" class="btn">Code</a>
      <a href="https://github.com/asifhkhan" class="btn">Dataset</a>
      <a href="data/default.bib" class="btn">Bibtex</a>
      </p>
    </div>
  </div>

  <!-- Publication 5 -->
  <div class="publication-item hover-video">
    <div class="video-container">
      <video muted autoplay loop>
        <source src="images/lbkenet2.mp4" type="video/mp4">
      </video>
      <img src="images/lbkenet.png" alt="LBKENet2 Paper Thumbnail">
    </div>
    <div class="publication-text">
      <a href="https://www.mdpi.com/2078-2489/14/5/296">
        <strong>Lightweight Implicit Blur Kernel Estimation Network for Blind Image Super-Resolution</strong>
      </a>
      <p> <strong><u>Asif Hussain Khan</u></strong>, Christian Micheloni, Niki Martinel</p>
      <p><em>Information Journal, 2023</em></p>
      <p>We propose a lightweight blind super-resolution (Blind-SR) model that estimates blur kernels and restores HR images without ground-truth supervision. Our method uses a Super Resolver and an Estimator Network trained with a novel loss for joint kernel and image recovery. We further extend our work to handle anisotropic Gaussian kernels for more complex degradations. Experiments show our approach is efficient and performs well with significantly fewer parameters than state-of-the-art models.</p>
      <p>
           <a href="https://github.com/asifhkhan" class="btn">Code</a>
      <a href="https://github.com/asifhkhan" class="btn">Dataset</a>
      <a href="data/default.bib" class="btn">Bibtex</a>
      </p>
    </div>
  </div>

</section>

  <!-- Achievements & Review Activities Section -->
<section class="section-container">
  <div class="achievements-review">

    <!-- Achievements -->
    <div class="achievements">
      <h2 id="Achievements">
        <i class="fas fa-trophy"></i> Achievements
      </h2>
      <ul>
        <li>PhD Fellowship Award – Fully funded doctoral fellowship granted by the Italian Ministry of Health-2023, in collaboration with the National Cancer Research Institute, CRO IRCCS.</li>
        <li>Master’s Scholarship – Tianjin University International Student Scholarship-2017,awarded for academic excellence.</li>
        <li> Laptop Award – Recipient under the Prime Minister’s Best Student Laptop Scheme 2015, Government of Pakistan.</li>
      </ul>
    </div>

    <!-- Review Activities -->
    <div class="review-activities">
      <h2>
        <i class="fas fa-clipboard-check"></i> Review Activities
      </h2>
      <ul>
        <li><strong>Conferences:</strong> WACV 2025, ICIAP 2025, ICIAP 202</li>
        <!-- <li><strong>Journals:</strong> Pattern Recognition, Image and Vision Computing</li> -->
      </ul>
    </div>

  </div>
</section>

<!--
<script>
document.addEventListener('DOMContentLoaded', () => {
  const themeIcon = document.getElementById('theme-icon');
  const navbar = document.querySelector('.navbar');
  const body = document.body;

  // Load saved theme
  const savedTheme = localStorage.getItem('theme');
  if (savedTheme === 'dark') {
    body.classList.add('dark-mode');
    themeIcon.classList.replace('fa-moon', 'fa-sun');
    themeIcon.title = "Toggle light mode";
  }

  // Dark mode toggle
  themeIcon.addEventListener('click', () => {
    body.classList.toggle('dark-mode');

    if (body.classList.contains('dark-mode')) {
      themeIcon.classList.replace('fa-moon', 'fa-sun');
      themeIcon.title = "Toggle light mode";
      localStorage.setItem('theme', 'dark');
    } else {
      themeIcon.classList.replace('fa-sun', 'fa-moon');
      themeIcon.title = "Toggle dark mode";
      localStorage.setItem('theme', 'light');
    }
  });

  // Navbar scroll effect
  window.addEventListener('scroll', () => {
    if (window.scrollY > 50) {
      navbar.classList.add('scrolled');
    } else {
      navbar.classList.remove('scrolled');
    }
  });
});
</script>

-->
    <script>
  document.addEventListener('DOMContentLoaded', () => {
    const navbar = document.querySelector('.navbar');
    const themeIcon = document.getElementById('theme-icon');
    const body = document.body;

    // Load saved theme
    const savedTheme = localStorage.getItem('theme');
    if (savedTheme === 'dark') {
      body.classList.add('dark-mode');
      themeIcon.classList.replace('fa-moon', 'fa-sun');
      themeIcon.title = "Toggle light mode";
    }

    // Dark mode toggle
    themeIcon.addEventListener('click', () => {
      body.classList.toggle('dark-mode');
      if (body.classList.contains('dark-mode')) {
        themeIcon.classList.replace('fa-moon', 'fa-sun');
        themeIcon.title = "Toggle light mode";
        localStorage.setItem('theme', 'dark');
      } else {
        themeIcon.classList.replace('fa-sun', 'fa-moon');
        themeIcon.title = "Toggle dark mode";
        localStorage.setItem('theme', 'light');
      }
    });

    // Navbar scroll effect
    window.addEventListener('scroll', () => {
      if (window.scrollY > 50) {
        navbar.classList.add('scrolled');
      } else {
        navbar.classList.remove('scrolled');
      }
    });
  });
  </script>

</body>
</html>

